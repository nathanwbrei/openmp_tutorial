\documentclass[]{beamer}
\usecolortheme{beaver}
\setbeamertemplate{navigation symbols}{}
\setbeamercolor{block title}{bg=white!95!black,fg=black}
\defbeamertemplate{description item}{align left}{\insertdescriptionitem\hfill}
    \setbeamertemplate{description item}[align left]
\usepackage{tikz}

\usepackage[outputdir=build]{minted}
\usepackage{tcolorbox}
\tcbuselibrary{minted,skins}

% Patch to fix https://github.com/T-F-S/tcolorbox/issues/12
\makeatletter
\def\tcb@minted@input@listing#1#2#3#4{%
  \edef\temp@a{#4}%
  \ifx\temp@a\@empty%
  \else%
    \toks@=\expandafter{#4}%
    \edef\tcb@temp{\noexpand\usemintedstyle{\the\toks@}}%
    \tcb@temp%
  \fi%
  \toks@=\expandafter{#1}%
  \edef\tcb@temp{\noexpand\inputminted[\the\toks@]}%
  \IfFileExists{\minted@outputdir#3}%
    {\tcb@temp{#2}{\minted@outputdir#3}}%
    {\tcb@temp{#2}{#3}}%
}
\makeatother

\newtcblisting{ccode}[2][]{%
  listing engine=minted,
  minted language=#2,
  minted options={breaklines,breakanywhere,fontsize=\scriptsize,gobble=8},
  listing only,
  before skip=0pt,
  after skip=8pt,
  left skip=0pt,
  right skip=0pt,
  size=fbox,
  sharp corners,
  %colframe=white!75!black,
  colframe=white,
  boxrule=0pt,
  frame hidden,
  #1
}


\newtcbinputlisting[]{\inputcode}[3][]{%
  listing engine=minted,
  minted language=#2,
  minted options={breaklines,breakanywhere,fontsize=\scriptsize,#1},
  listing file={#3},
  listing only,
  size=fbox,
  before skip=0pt,
  after skip=10pt,
  left skip=0pt,
  right skip=0pt,
  sharp corners,
  %colframe=white!75!black,
  colframe=white,
  boxrule=0pt,
  frame hidden
}


\title{Introduction to OpenMP}
\subtitle{Ferienakademie 2017}
\author{Nathan Brei}
\institute{Technical University of Munich}
\date\today

\begin{document}
\begin{frame}
  \titlepage
\end{frame}

%\begin{frame}
%  \frametitle{Parallelism levels}
%  \begin{block}{Distributed-memory parallelism}
%    \begin{description}[Disadvantages]
%    \item[Description] Isolated processes, often running on different machines, communicate by sending messages.
%    \item[Advantages] High scalability
%    \item[Disadvantages] Reliability, communication overhead
%    \item[Examples] MPI, Hadoop, Spark. Akka, Erlang. 
%    \end{description}
%  \end{block}

%  \begin{block}{Shared-memory thread-level parallelism}
%    \begin{description}[Disadvantages]
%    \item[Description] Multiple threads on the same machine. Each thread has its own isolated register space, but they share a memory address space.
%    \item[Advantages] Parallelizing existing code, memory-bound problems
%    \item[Disadvantages] Contains subtleties
%    \item[Examples] Pthreads, Java threads, OpenMP
%    \end{description}
%  \end{block}
%\end{frame}

\begin{frame}
  \frametitle{Parallelism levels}
  \begin{block}{Instruction-level parallelism}
    \begin{description}[Disadvantages]
    \item[Description] Each CPU core maximizes instruction throughput by pipelining, parallelizing execution units and registers, etc. Shared registers, shared memory.
    \item[Advantages] Vectorize innermost compute kernels, use memory hierarchy efficiently
    \item[Disadvantages] Limited scalability, absurdly low-level
    \item[Examples] Assembly language, \texttt{\#pragma simd}, choosing block sizes very carefully
    \end{description}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Introduction to OpenMP}

  \begin{block}{Goals}
    \begin{itemize}
    \item Higher-level threading concepts/patterns
    \item Faster, less boilerplate, fewer threading errors
    \item Convenient for parallelizing existing code
    \item Automatic fallback to sequential code
    \item Automatically chooses parallelism level at runtime
    \item Can be used in conjunction with MPI for further scaling
    \end{itemize}
  \end{block}

  \begin{block}{Tradeoffs}
    \begin{itemize}
    \item Implemented as compiler directives. Opaque internals.
    \item Simple syntax does not constrain semantics.
    \end{itemize}
  \end{block}

\end{frame}


\begin{frame}[fragile]
  \frametitle{Launching a team of threads --- Parallel Regions}
  \begin{columns}[t]%[onlytextwidth]
    \begin{column}{0.65\textwidth}
      \inputcode[]{c}{src/ex1.c}
    \end{column}
    \begin{column}{0.4\textwidth}
      \inputcode[]{text}{output/ex1.txt}
      \begin{ccode}[]
        {text}
        About to fork...
        Hello from thread 2 of 3
        Hello from thread 0 of 3
        Hello from thread 1 of 3
        ...Rejoined.
      \end{ccode}
    \end{column}
  \end{columns}
  \begin{itemize}
  \item \emph{Fork-join execution model}
  \item Print statements are interleaved nondeterministically
  \item Number of threads is chosen automatically, or specified by \mintinline{text}{$OMP_NUM_THREADS}, \mintinline{c}{omp_set_num_threads(int t)}, \mintinline{c}{#pragma omp parallel num_threads(2)}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Assigning code blocks to threads --- Parallel Sections}
  \begin{columns}[t]%[onlytextwidth]
    \begin{column}{0.65\textwidth}
      \inputcode[]{c}{src/ex2.c}
    \end{column}
    \begin{column}{0.4\textwidth}
      \begin{ccode}[]
        {text}
        Thread 0, iter 0
        Thread 0, iter 1
          Thread 1, iter 0
        Thread 0, iter 2
          Thread 1, iter 1
          Thread 1, iter 2
          Thread 1, iter 3
        Thread 0, iter 3\end{ccode}
      \inputcode[]{text}{output/ex2.txt}
    \end{column}
  \end{columns}
  \begin{itemize}
  \item Number of parallel sections is fixed at compile time.
  \item No guarantee how statements get interleaved, or how many threads are actually used!
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Problem: Race conditions}
  Definition: A class of bug where the program's output depends on the timing of events which are not under the programmer's control.  
  \begin{columns}[t]%[onlytextwidth]
    \begin{column}{0.5\textwidth}
      \begin{ccode}[]{c}
        int a = 0;
        int b = 0;
        #pragma omp parallel sections
        {
          #pragma omp section
          a = 1;

          #pragma omp section
          b = a;
        }
        printf("b = %d\n", b);\end{ccode}
    \end{column}
    \begin{column}{0.3\textwidth}
      \begin{ccode}[]
        {text}
        a = 0
        b = 0
        a = 1
        b = a
          ==> (b == 1)\end{ccode}
      \begin{ccode}[]
        {text}
        a = 0
        b = 0
        b = a
        a = 1
          ==> (b == 0)\end{ccode}
    \end{column}
  \end{columns}
  \begin{itemize}
  \item For simple programs, the output often \emph{appears} deterministic.
  \item However, instruction interleaving is \emph{not} defined by OpenMP.
  \item It depends on the CPU architecture, operating system, system load, compiler, and code optimization level.
  \item Use Valgrind or ThreadSanitizer to proactively find dormant race conditions.
  \end{itemize}

\end{frame}



\begin{frame}[fragile]
  \frametitle{Problem: Race conditions}
  Some race conditions are not obvious from the C code, but emerge when compiled to assembly/microcode. Assume \emph{every} variable access in C requires memory \texttt{load} or \texttt{store} operations unless the compiler can prove otherwise. 
  
  \begin{columns}[t]%[onlytextwidth]
    \begin{column}{0.43\textwidth}
      \inputcode[firstline=5,lastline=22,gobble=2]{c}{src/ex4.c}
    \end{column}
    \pause

    \begin{column}{0.43\textwidth}
      \inputcode[firstline=5,lastline=22,gobble=2]{c}{src/ex5.c}
    \end{column}
    
    \begin{column}{0.3\textwidth}
      \begin{ccode}[after skip=4pt]
        {text}
        load (total) -> a
          load (total) -> c
        add 1, a -> b
          add 2, c -> d
        store b -> (total)
          store d -> (total)

          ==> (total == 2)\end{ccode}
      \begin{ccode}[]
        {text}
        load (total) -> a
        add 1, a -> b
          load (total) -> c
          add 2, c -> d
          store d -> (total)
        store b -> (total)
        
          ==> (total == 1)\end{ccode}
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Dependency Graphs}

  How do we know if a parallel program is correct? 

  Which instruction reorderings preserve the program's meaning?

  \begin{itemize}

  \item Analyze data flow and control flow to find a minimal partial ordering, a.k.a.\ a \emph{dependency graph}.

  \item Two instructions have a data dependency if both access the same variable and at least one access is a write.

  \item Kinds of data dependencies: Read-After-Write, Write-After-Read, Write-After-Write

  \item WAR and WAW hazards can sometimes be finessed away by simply referencing a different memory location.

  \end{itemize}

      \begin{ccode}[]
        {c}
        1: x = 22;
        2: y = x + 1;     // RAW dep on (1)
        3: x = 0;         // WAR dep on (2)
        4: y = 0;         // WAW dep on (3)
      \end{ccode}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Privatizing intermediate variables}
  \begin{columns}[t]%[onlytextwidth]
    \begin{column}{0.6\textwidth}
      \inputcode[firstline=8,lastline=32,gobble=2]{c}{src/ex6.c}
    \end{column}

    \begin{column}{0.4\textwidth}
      \inputcode[]{text}{output/ex6.txt}
      Problem: Compiler treats variables as shared by default! Reusing variable names creates unnecessary data hazards.
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Privatizing intermediate variables}
  \begin{columns}[t]%[onlytextwidth]
    \begin{column}{0.6\textwidth}
      \inputcode[firstline=8,lastline=32,gobble=2]{c}{src/ex7.c}
    \end{column}
    \begin{column}{0.4\textwidth}
      \inputcode[]{text}{output/ex7.txt}
      Distinguish between variables which \emph{must} be shared and those which should be scoped local to a thread. Use \texttt{firstprivate, lastprivate} to share with enclosing scope. 
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Protecting shared resources -- Critical and Atomic Sections}
  \begin{columns}[t]
    \begin{column}{0.7\textwidth}
      \inputcode[firstline=6,lastline=28,gobble=2]{c}{src/ex8.c}
    \end{column}
    \begin{column}{0.4\textwidth}
      \inputcode[]{text}{output/ex8.txt}
      Mark a code block for mutually exclusive access. This protects a (possibly named) resource using a lock. Long critical regions hurt scalability (see Amdahl's Law). For operations like \texttt{+=}, use \texttt{\#pragma omp atomic}, which protects the loads/stores at the instruction level.
      
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Enforcing statement ordering between threads --- Barriers}
  \begin{columns}[t]%[onlytextwidth]
    \begin{column}{0.6\textwidth}
      \inputcode[firstline=5,lastline=27,gobble=2]{c}{src/barrier.c}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{block}{Definition}
      A point in the execution of a program encountered by a team of threads, beyond which no thread in the team may execute until all threads in the team have reached the barrier.
      \end{block}
      \begin{block}{In practice}
          Implicit barriers are defined at the end of most work-sharing constructs, such as \texttt{sections}, unless \texttt{nowait} is specified. For complex orderings, use \texttt{for ordered}.
      \end{block}
    \end{column}
  \end{columns}
\end{frame}



\begin{frame}[fragile]
  \frametitle{Parallelizing For Loops}
  For numerical code, most of the computation usually happens inside an inner loop, and we wish to distribute the work evenly across an arbitrary number of threads determined at runtime.
  \begin{columns}[t]%[onlytextwidth]
    \begin{column}{0.65\textwidth}
      \inputcode[]{c}{src/exfor.c}
    \end{column}
    \begin{column}{0.4\textwidth}
      \inputcode[]{text}{output/exfor.txt}
    \end{column}
  \end{columns}
  \begin{itemize}
  \item Available scheduling strategies: static, dynamic, guided
  \item Adjust chunk size in order to increase the parallelism granularity
  \end{itemize}
\end{frame}



\begin{frame}[fragile]
  \frametitle{Loop-Carried Dependencies}
  \begin{columns}[t]%[onlytextwidth]
    \begin{column}{0.55\textwidth}
      \inputcode[firstline=6,lastline=15,gobble=2]{c}{src/lcd.c}
    \end{column}
    \begin{column}{0.55\textwidth}
      \inputcode[firstline=6,lastline=15,gobble=2]{c}{src/lcdbroken.c} 
    \end{column}
  \end{columns}
  \begin{itemize}
  \item A loop may be parallelized if there are no dependencies between statements in different iterations (``loop-carried dependencies'').
  \item Simple heuristic: Could we implement using \mintinline{c}{map()}?
  \item For nested loops: Inner loop has the loop-carried dependencies (so they run in one thread), outer loop gets parallelized.
  \item Techniques for rearranging loops: loop splitting, alignment, interchange 
  
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Loop-Carried Dependencies}
  \begin{columns}[t]
    \begin{column}{0.55\textwidth}
      \inputcode[firstline=6,lastline=15,gobble=2]{c}{src/lcd.c}
      \inputcode[firstline=6,lastline=16,gobble=2]{c}{src/lcd2.c}
    \end{column}
    \begin{column}{0.55\textwidth}
      \inputcode[firstline=6,lastline=15,gobble=2]{c}{src/lcdbroken.c} 
      \inputcode[firstline=6,lastline=15,gobble=2]{c}{src/lcdbroken2.c} 
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Reductions: Sum all elements in array}
  \inputcode[firstline=6,lastline=14]{c}{src/reduction.c}
  \begin{itemize}
  \item OpenMP creates a private copy of the reduction variable \texttt{sum} for every thread.
  \item Each thread will apply the reduction operation, accumulating into the local reduction variable.
  \item Once the threads complete, OpenMP applies the reduction operator to the local reduction variables, accumulating into the global reduction variable.
  \item Reductions are of the form \texttt{+=} or \texttt{min()}, custom reductions possible.
  \end{itemize}
\end{frame}

\begin{frame}[plain,c]
\begin{center}
    \huge Thank you!
\end{center}
\end{frame}

\end{document}
