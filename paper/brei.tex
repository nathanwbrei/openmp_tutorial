\documentclass[conference, a4paper]{IEEEtran-modified}
\usepackage{tikz}

\usepackage[outputdir=build]{minted}
\usepackage{tcolorbox}
\tcbuselibrary{minted,skins}

% Patch to fix https://github.com/T-F-S/tcolorbox/issues/12
\makeatletter
\def\tcb@minted@input@listing#1#2#3#4{%
  \edef\temp@a{#4}%
  \ifx\temp@a\@empty%
  \else%
    \toks@=\expandafter{#4}%
    \edef\tcb@temp{\noexpand\usemintedstyle{\the\toks@}}%
    \tcb@temp%
  \fi%
  \toks@=\expandafter{#1}%
  \edef\tcb@temp{\noexpand\inputminted[\the\toks@]}%
  \IfFileExists{\minted@outputdir#3}%
    {\tcb@temp{#2}{\minted@outputdir#3}}%
    {\tcb@temp{#2}{#3}}%
}
\makeatother

\newtcblisting{ccode}[2][]{%
  listing engine=minted,
  minted language=#2,
  minted options={breaklines,breakanywhere,fontsize=\scriptsize,gobble=8},
  listing only,
  before skip=5pt,
  after skip=5pt,
  left skip=0pt,
  right skip=0pt,
  size=fbox,
  sharp corners,
  %colframe=white!75!black,
  colframe=white,
  boxrule=0pt,
  frame hidden,
  #1
}


\newtcbinputlisting[]{\inputcode}[3][]{%
  listing engine=minted,
  minted language=#2,
  minted options={breaklines,breakanywhere,fontsize=\scriptsize,#1},
  listing file={#3},
  listing only,
  size=fbox,
  before skip=5pt,
  after skip=5pt,
  left skip=0pt,
  right skip=0pt,
  sharp corners,
  %colframe=white!75!black,
  colframe=white,
  boxrule=0pt,
  frame hidden
}


\title{A Brief Introduction to Shared-Memory Parallelization using OpenMP}

\specialpapernotice{Ferienakademie 2017}

\author{
\authorblockN{Nathan W Brei}
\authorblockA{\texttt{brei@in.tum.de}} 
%\and
%\authorblockN{}
%\authorblockA{}
}
\date\today
\pagestyle{plain}

\begin{document}
\maketitle

This document provides a concise introduction to parallelizing a program using OpenMP. OpenMP is an API for shared-memory parallelism, distributing the computation across multiple threads which all access the same address space. It is designed to allow a programmer to easily adapt existing sequential code by annotating blocks which can be parallelized. Each annotation corresponds to a high-level pattern. This paper introduces the most essential patterns, specifically parallel regions, parallel sections, critical/atomic sections, parallel for loops, barriers, and reductions. Along the way it discusses fundamental parallel programming concepts such as race conditions and dependency graphs. In the end, this paper shows how the API may be used to parallelize a Lattice-Boltzmann fluid simulation.

\section{Parallelism basics}

There are two main reasons to parallelize a program: to make it faster, and to handle larger problem sizes. The measure of effectiveness is usually \emph{speedup} $ S(n) = t_1 / t_n$, where $n$ denotes the number of resources, e.g. processors, and $t$ denotes time taken. If a program parallelizes perfectly, then $S(n) = n$. This is only the case for problems which can be divided into completely independent subproblems, which are called \emph{embarrassingly parallel}. Otherwise, the speedup saturates so that adding more resources yields diminishing returns. This is modelled as \emph{Amdahl's Law}, 

\[
S(n) = \frac{1}{1-f+f/n}
\]

where $f$ denotes the fraction of the computation which can be fully parallelized. As $n$ is increased, the runtime becomes dominated by the parts of the program which do not benefit from the parallelism.

When designing parallel programs, it is helpful to imagine an assembly line. Although work may be divided between different processors, some operations must be performed sequentially, creating a \emph{critical path} which acts as an upper bound on the attainable performance, called the \emph{span}. Optimizing operations which are not on the critical path does not improve performance. Thus the bulk of a program does not need to be optimized, but rather just the \emph{bottlenecks}. There are two main kinds of bottlenecks: a \emph{compute bound} is limited by the throughput of executing instructions, whereas a \emph{memory bound} is limited by bandwidth of moving memory into the CPU. According to the \emph{Roofline Model}, these can be distinguished by calculating the number of floating-point operations per byte, known as the \emph{arithmetic intensity}. The general procedure is to identify the key bottleneck and reorganize it until it is no longer the key bottleneck, then repeat.


\section{Shared-memory versus distributed-memory parallelism}

    A program may be parallelized at different levels. \emph{Distributed-memory parallelism} spreads the work across multiple processors which must communicate by sending each other messages. \emph{Thread-level (shared-memory) parallelism} spreads the work across multiple threads within one processor. \emph{Instruction-level parallelism} maximizes the throughput of instructions within the context of each thread. Each of these levels comes with a different set of tradeoffs and tools. They may usually be combined, although this increases the complexity of the program.

\subsection{Distributed-memory parallelism}

    Distributed-memory parallelism splits the computation across multiple processors. These processors may be located within the same physical machine, or they may be geographically far apart. In either case, the program is run as a group of isolated processes which interact by passing messages. The most commonly used framework in scientific computing is MPI, while in other fields frameworks like Hadoop, Spark, and Erlang are popular.

    The main advantage to distributed-memory parallelism is that it permits high scaling. While a single processor may offer compute power in the low teraflops, a supercomputer scales this to the high petaflops. The performance of a single processor is limited by geometry and the physics of heat dissipation, whereas the performance of a cluster is limited mainly by the cost of communication overhead. This reveals a key disadvantage: a network hop is orders of magnitude more expensive than the corresponding memory lookup, leading to a much lower memory bound. The second disadvantage is reliability: as the number of machines increases, so too does the probability that one of the machines fails. A long-running program must account for this; the simplest option is to periodically save a checkpoint containing the program's state. Frameworks such as Spark automatically detect failures and rearrange the computation to compensate. 


\subsection{Instruction-level parallelism}

    Instruction-level parallelism happens within the context of a single core, and is specific to each processor's microarchitecture. On one hand, each core maximizes its own instruction throughput by pipelining and parallelizing functional units. On the other hand, the use of \emph{vectorized} instructions allows one instruction to operate on multiple values at once. Taking advantage of this parallelism level requires detailed knowledge of the hardware, so it is usually delegated to the compiler. The programmer usually coaxes the compiler to find an efficient solution by reorganizing data structures and providing hints. Although this maximizes the utilization of each core, there is a limited scalability gain (e.g. a speedup of 8) and even this requires a predominantly compute-bound problem. Tools for exploiting instruction-level parallelism include  pragmas, intrinsics functions, and plain assembly code.


\subsection{Shared-memory thread-level parallelism}

    Thread-level parallelism splits the computation across multiple threads within a single processor. This can yield a speedup when each thread is executed on a separate core. Each thread sees its own isolated register space, but all threads access the same memory address space. The standard POSIX threading library is called pthreads, but it is too low-level for everyday use. Two popular higher-level libraries are OpenMP and Thread Building Blocks (TBB). OpenMP extends C/C++/Fortran using pragmas, whereas TBB is built from C++ templates.

    The main advantage to thread-level parallelism is that the same data is automatically available to all threads. This makes it the only parallelism level which is effective for memory-bound problems: distributed-memory parallelism narrows the memory bottleneck, and instruction-level parallelism is overshadowed by it. 

    A second advantage is that it allows an incremental approach to parallelizing existing code. In contrast, distributed-memory parallelism requires reorganizing the program's entire control flow, and instruction-level parallelism often requires reorganizing the data structures. OpenMP, in particular, is built into the language itself so that it can rewrite entire syntax trees. This means that blocks of code can be parallized in place by merely adding an annotation; the work of managing the team of threads is handled transparently. 

    The main disadvantage of thread-level parallelism is that it is conceptually tricky -- for both a human and a compiler -- to reason about program correctness and liveness. A program may intermittently return incorrect results or hang, depending on the exact pattern of data updates. The high-level parallelism directives can not completely isolate the effects of their enclosed computations. In order to write correct parallel code, a programmer must have a thorough understanding of the sequential code. Furthermore, flaws usually cannot be detected at compile time. Tools such as ThreadSanitizer \emph{instrument} the binary in order to detect them at runtime.


\section{Introduction to OpenMP}

    OpenMP provides high-level threading constructs as a language extension to C, C++, and Fortran. It develops a set of parallel programming patterns and allows the user to apply these patterns throughout an existing codebase.

    Tackling the problem at a higher level allows OpenMP to assuage many of the human-factors problems associated with multithreaded programming. The boilerplate for managing threads is long, tedious, and error-prone. Because this code has been consolidated, it can handle edge cases thoroughly and with high generality. For instance, not only is it is possible to set the number of threads at runtime, but the optimal number can be determined automatically by querying the operating system. This implies that OpenMP offers automatic fallback to sequential code.  

    There are several downsides associated with using OpenMP. Due to the use of compiler directives, the parallelized version of the code is kept hidden from the user. The only way to understand the transformations OpenMP applied to the code is by using a specialized debugger, or reading the assembly. This can be immediately compared with languages such as Julia, which implement similar functionality using macros, thereby allowing the code to be inspected after the transformations are applied but before they are compiled.

    A second downside is that the OpenMP syntax remains simple even if the resulting transformation is quite complex. To a programmer, this means that parallelizing each code region is an all-or-nothing task. To a compiler, this means that there can be very limited error detection. It can detect when directives are nested in a way which does not make sense, or when the arguments to a clause are invalid. However, there remains a huge space of programs which are syntactically valid but semantically incorrect. OpenMP by its very nature transforms the meaning of a program; it does not attempt to preserve that program's correctness in the process.

    The OpenMP syntax has a simple grammar, as shown by the example below:
      \begin{ccode}[]
        {text}
        #pragma omp parallel private(acc) num_threads(8)
      \end{ccode}

    \texttt{\#pragma} indicates that the rest of the line is not C code, but rather an optional hint for the compiler. \texttt{omp} is a \emph{sentinel} indicating that this is an OpenMP directive. \texttt{parallel} is the directive name, which in this case indicates that the following block of code should be run in parallel using a team of threads, as described in the next section. The directive name is followed by a set of clauses. Clauses are separated by spaces, may be in any order, and may be repeated. Many clauses take parameters, which are enclosed in parentheses following the clause name; arguments are separated by commas and keyworded arguments by colons, similar to Python.
    


\section{Launching a team of threads --- Parallel Regions}

    The most basic OpenMP construct is the \emph{parallel region}, which is a single block of code that gets executed once by every thread in the team. This implements the \emph{fork-join execution model}, in which control flow starts in a single thread, forks to multiple threads, all of which perform some computation, and then rejoins upon completion, with one thread continuing execution and the rest either idling or terminating. The following example shows an extremely simple block run within a parallel region that has been constrained to use three threads:

      \inputcode[]{c}{src/ex1.c}

    The \texttt{num\_threads} clause fixes the number of threads in the team. This should only be hardcoded like this when the correctness of the enclosed code depends on that number, since hardcoding prevents the program from taking advantage of additional cores which may be available on different machines. In order to set the value at runtime from within the program, OpenMP also provides the function \mintinline{c}{omp_set_num_threads(int t)}. To set the value at runtime from outside the program it is convenient to use the environment variable \mintinline{c}{OMP_NUM_THREADS=3}. If the variable is not constrained anywhere, OpenMP will query the operating system in order to determine the best value. 

    Upon being run, the code produces the following output:

      \inputcode[]{text}{output/ex1.txt}


    It is important to note that the OpenMP specification does not guarantee that three threads are used in practice, or even that the code is run in parallel at all. If the implementation decides that the best option is to run the code sequentally within one thread three times, it is free to do so. This is for practical reasons: the mapping from threads to cores is buried beneath many layers, including the OpenMP implementation, the operating system, and the CPU architecture, all of which may vary independently and offer their own notion of a virtualized thread.

    This does not pose a theoretical problem because while instructions within a thread are required to be ordered, there is corresponding assumption between threads. Instead, they are interleaved nondeterministically. When run a second time, the following output is produced: 

      \begin{ccode}[]
        {text}
        About to fork...
        Hello from thread 2 of 3
        Hello from thread 0 of 3
        Hello from thread 1 of 3
        ...Rejoined.
      \end{ccode}

    Note however that the interleaving is also not always obviously random. Small examples, such as those in this paper, often run with the same interleaving many times in a row, and different interleavings only manifest when run on different hardware or on a larger problem size.


\section{Sections}


Oftentimes a programmer wishes to assign distinct code blocks to individual threads. While this could technically be accomplished by using a plain parallel region and branching according to thread number, a higher-level construct, called a \emph{section}, achieves this in a general way. 

The syntax is as follows: Within a \texttt{parallel} region, the programmer may declare a \texttt{sections} region. Within the latter, they may then declare any number of individual \texttt{section} blocks. Each \texttt{section} is guaranteed to be executed exactly once, and OpenMP automatically decides how sections get mapped to threads. Note that the number of sections is fixed at compile time. For brevity the \texttt{parallel} and \texttt{sections} directives may be combined as shown in the example below: 

      \inputcode[]{c}{src/ex2.c}
      \begin{ccode}[]
        {text}
        Thread 0, iter 0
        Thread 0, iter 1
          Thread 1, iter 0
        Thread 0, iter 2
          Thread 1, iter 1
          Thread 1, iter 2
          Thread 1, iter 3
        Thread 0, iter 3\end{ccode}
      \inputcode[]{text}{output/ex2.txt}


At the end of a \texttt{sections} region is an \emph{implicit barrier}.
One tool which considerably simplifies reasoning about parallel programs is 

    In order to preserve 


  One common source of problems happens when 

  Related problem: different threads take different paths.
  Protect control flow. Enforce ordering between threads. Barriers.

  Definition: A point in the execution of a program encountered by a team of threads, beyond which no thread in the team may execute until all threads in the team have reached the barrier.
 
  In practice: Implicit barriers are defined at the end of most work-sharing constructs, such as \texttt{sections}, unless the \texttt{nowait} clause is specified. For complex orderings, use \texttt{for ordered}.

  When can barriers be used?

  


\section{Race conditions}

The previous two examples demonstrated that instructions are only \emph{partially ordered}: Within each thread all instructions are ordered, but between threads they may be interleaved nondeterministically like a shuffled deck of cards. If multiple threads access a shared resource, the ordering of those accesses is unpredictable. This makes multithreaded programs vulnerable to \emph{race conditions}, a class of bug where the program's output depends on the timing of events which are not under the programmer's control. The following code snippet shows a variable \texttt{b} being assigned the value of \texttt{a}, while \texttt{a} is being simultaneously updated. Whether the new value of \texttt{a} propagates into \texttt{b} is indeterminate.


      \begin{ccode}[]{c}
        int a = 0;
        int b = 0;
        #pragma omp parallel sections
        {
          #pragma omp section
          a = 1;

          #pragma omp section
          b = a;
        }
        printf("b = %d\n", b);\end{ccode}

      \begin{ccode}[]
        {text}
        b = 1\end{ccode}
      \begin{ccode}[]
        {text}
        b = 0\end{ccode}

For simple programs, the output often appears deterministic. However, instruction interleaving is \emph{not} defined by OpenMP. It depends on the CPU architecture, operating system, system load, compiler, and code optimization level. Use Valgrind or ThreadSanitizer to proactively find dormant race conditions.

Oftentimes race conditions are not obvious from the C code, but emerge from the resulting assembly or microcode. One major reason is that the compiler determines when data gets moved between memory and registers. The programmer must assume that every variable access requires separate \texttt{load} and \texttt{store} operations which may be vulnerable to interleaving. It is possible to avoid this by using \emph{atomic} operations, but this is very inefficient, for reasons which shall be explored later. 

The following C code updates a shared accumulator, \texttt{total}, in two separate threads. A naive analysis suggests that this code will always return 3, regardless of interleaving, because integer addition is associative.

%  \inputcode[firstline=5,lastline=22,gobble=2]{c}{src/ex4.c}
        \begin{ccode}[]{c}
        #include<stdio.h>
        #include<omp.h>
        int main(int argc, char** argv) {

          int total = 0;
          #pragma omp parallel sections
          {
            #pragma omp section
            {
              total = total + 1;
            }

            #pragma omp section
            {
              total = total + 2;
            }
          }
          printf("total=%d\n",total);
        }
\end{ccode}



However, once each statement has been broken into separate \texttt{load}, \texttt{add}, and \texttt{store} statements, it becomes clear that there exist interleavings which are not correct. To run correctly, the instruction ordering must be constrained such that one \texttt{load} comes after the other \texttt{store} instruction; otherwise the effect of one of the statements will be overwritten completely.

  \inputcode[firstline=5,lastline=22,gobble=2]{c}{src/ex5.c}
  
      \begin{ccode}[after skip=4pt]
        {text}
        load (total) -> a
          load (total) -> c
        add 1, a -> b
          add 2, c -> d
        store b -> (total)
          store d -> (total)

          ==> (total == 2)\end{ccode}
      \begin{ccode}[]
        {text}
        load (total) -> a
        add 1, a -> b
          load (total) -> c
          add 2, c -> d
          store d -> (total)
        store b -> (total)
        
          ==> (total == 1)\end{ccode}

  \section{Dependency Graphs}

To determine whether or not a parallel program is correct, one must determine whether there exists a reordering which changes the program's meaning. 


Analyze data flow and control flow to find a minimal partial ordering, a.k.a.\ a \emph{dependency graph}.

Two instructions have a data dependency if both access the same variable and at least one access is a write.

There are three kinds of data dependencies: Read-After-Write (RAW), Write-After-Read (WAR), and Write-After-Write (WAW). WAR and WAW hazards can sometimes be finessed away by simply referencing a different memory location.


      \begin{ccode}[]
        {c}
        1: x = 22;
        2: y = x + 1;     // RAW dep on (1)
        3: x = 0;         // WAR dep on (2)
        4: y = 0;         // WAW dep on (3)
      \end{ccode}


\section{Privatizing intermediate variables}

  One very common source of errors in OpenMP programs is that the compiler treats variables as shared by default. In other words, if a variable is declared outside of the \texttt{parallel} or \texttt{sections} directive, and is used inside it, the compiler assumes that the symbol refers to the same memory location throughout. In some cases, particularly with iteration or temporary variables, this creates a spurious dependence. In the following code snippet, one thread attempts to count from 0 to 3 and the other attempts to count from 4 to 7. The variables $i$ and $t$ are declared outside of the \texttt{sections} directive. 


  \inputcode[firstline=8,lastline=32,gobble=2]{c}{src/ex7.c}

  The correctness of this code hinges on the clause \mintinline{c}{private(i,t)}. When included, the program runs as expected:

  \inputcode[]{text}{output/ex7.txt}

  When the clause is omitted, the threads overwrite the variables in a chaotic way, causing the loops to terminate early:

  \inputcode[]{text}{output/ex6.txt}

  When writing new code, it is good practice to declare thread-local variables within the block itself and to avoid writing to global state. With existing code, this would require significant refactoring and is sometimes not even possible. The alternative is to explicitly tell OpenMP which variables are meant to be shared and which are meant to be isolated. There are several clauses for this purpose. By default, OpenMP assumes that all variables are shared, and the programmer must declare them as \mintinline{c}{private}. This default assumption may be changed, via the \mintinline{text}{default} clause, which accepts as arguments \mintinline{text}{shared} or  
  mintinline{c}{none}. When the default is changed to \mintinline{text}{none}, all variables within the lexical extent of that parallel region must be explicitly scoped. 

  There are some subtleties involved when scoping a variable as \mintinline{text}{private}. Under the hood, OpenMP is declaring a new object of the same type for each thread and changing all references within the block to refer to it. However, it neither initializes the local variables nor updates the global variable after the parallel region has exited. For the former, OpenMP provides the \mintinline{c}{firstprivate} clause, which copies the global value into each local variable. For the latter, it provides the \mintinline{c}{lastprivate} clause, which assigns the local value from the last loop iteration or section to the global variable. This mimics the effects of the sequential version of the code. The two clauses may be combined.

\section{Protecting shared resources -- Critical and Atomic Sections}

  While it is desirable to keep variables private to a thread whenever possible, results must be written to a shared variable at some point, as otherwise the results of the computation will be lost when the threads join. The standard tools for achieving this are locks and mutexes. The basic idea is that each shared variable (or `resource') is protected by a lock variable. Each thread should acquire the lock before modifying the resource and release it afterwards. If one thread has already acquired the lock, the other threads must wait until it is released. This basic pattern forms the core of the \texttt{pthreads} library. 

  Unfortunately, using locks in practice is difficult to reason about and prone to bugs. OpenMP provides an interface for working with locks if necessary, but it strongly prefers a higher-level construct called a \emph{critical section}. Critical sections operate on explicit code blocks, rather than on the abstract notion of a resource. OpenMP sets up locks under the hood to ensure that only one thread may be inside a critical section at any given time. The programmer only needs to make sure that all writes to the shared resource are inside critical sections, and the resource will be protected.

  The downside to using critical sections is that they effectively serialize part of the program. As per Amdahl's Law, the larger the critical sections, the worse the program will scale. Thus it is desirable to keep them as short and as few as possible. If there are multiple resources which can be updated independently, it is beneficial to use \emph{named} critical sections for each resource, so that updates to one resource don't unnecessarily block updates to another.

  Oftentimes the critical sections are extremely short, possibly only several assembly instructions, such as the summation operator \texttt{+=}. In this case, it is overkill to acquire a lock since the synchronization can be achieved by protecting the load and store at the assembly level instead. This is much finer-grained and lightweight. However, it may only be used on statements with unary operators such as \mintinline{c}{++, --} or binary operators such as \mintinline{c}{+=, *=, -=, /=, &=, ^=, |=, <<=, >>=}. 

  Use of a named critical section and atomic section are shown below.

    \inputcode[firstline=6,lastline=28,gobble=2]{c}{src/ex8.c}

    \inputcode[]{text}{output/ex8.txt}




\section{Parallel For Loops}

  For numerical code, the majority of the computation usually happens inside a loop, and the programmer wishes to distribute the work evenly across the machine's cores. This should not be achieved with sections because sections use a fixed number of threads and would require code duplication for each thread. Instead, OpenMP provides a directive to parallelize a loop as follows: 

      \inputcode[]{c}{src/exfor.c}
      \inputcode[]{text}{output/exfor.txt}
      
  There are three scheduling strategies: static, dynamic, and guided. Static scheduling assigns each thread a chunk of work in a round-robin fashion. 

  These may be chosen at runtime. When choosing a strategy and chunk size, there are two main considerations: Smaller chunk sizes lead to better load balancing, but more granular parallelism, which increases overhead. 


\section{Loop-Carried Dependencies}

  The dependency analysis described earlier can be extended to handle the case of loops. There are two main goals: firstly, to determine whether a loop can be parallized as-is, and secondly, if not, to find a transformation which allows it to be parallelized efficiently.

      \inputcode[firstline=6,lastline=15,gobble=2]{c}{src/lcd.c}
      \inputcode[firstline=6,lastline=15,gobble=2]{c}{src/lcdbroken.c} 

  A loop may be parallelized if there are no dependencies between statements in different iterations, known as \emph{loop-carried dependencies}. There is a simple heuristic: Could one implement the loop using the \mintinline{c}{map()} function

        \begin{ccode}[]
        {haskell}
        map :: (x -> y) -> [x] -> [y]
      \end{ccode}


  If so, the loop can be parallelized as-is. If not, it may be possible to apply certain loop transformations.

  One useful technique is \emph{loop splitting}, which breaks a loop into two loops, one following the other. 

  Another useful technique is \emph{loop alignment}. Loop alignment reorganizes 

  With nested loops, there is the possibility of \emph{loop interchange}, where the nesting order is rearranged. There are two goals which must be considered: to keep loop-carried dependencies within each thread (so that they run sequentially and hence correctly), and to assign each thread as much work as possible for each iteration (to eliminate overhead). If the loop nest is two deep, there are four possible choices. The following example computes the sum of all rows in a matrix.

      \inputcode[firstline=6,lastline=15,gobble=2]{c}{src/lcd.c}
      \inputcode[firstline=6,lastline=16,gobble=2]{c}{src/lcd2.c}

      \inputcode[firstline=6,lastline=15,gobble=2]{c}{src/lcdbroken.c} 
      \inputcode[firstline=6,lastline=15,gobble=2]{c}{src/lcdbroken2.c} 

  The $i$ loop has loop-carried dependencies, whereas the $j$ loop does not. Thus the \texttt{parallel for} directive should be applied to the $j$ loop; otherwise the program would be incorrect. If the $j$ loop is on the inside, there are $m$ parallel forks/joins, each of which performs one instruction. On the other hand, if it is outermost, there is only one parallel fork/join, which performs $m$ operations. Thus one choice stands out as best.


\section{Reductions}

While \emph{embarrassingly parallel} loops fit the pattern used by the \mintinline{c}{map()} function, where each iteration in the loop is completely independent, other loops more closely resemble its counterpart \mintinline{c}{reduce()}, where an accumulator is updated with a new piece of data on every iteration. The canonical example of a reduction is summing the elements in an array. Both of these patterns can be understood from their type signatures: \mintinline{c}{reduce()} takes a combining operation, an initial value for the accumulator, and a sequence of values, and returns the accumulator after it has been combined with the entire sequence.

      \begin{ccode}[]
        {haskell}
        map :: (x -> y) -> [x] -> [y]
        reduce :: (a -> x -> a) -> a -> [x] -> a
      \end{ccode}

Although a reduction is practically the definition of a loop-carried dependence, it may nonetheless be parallelized as long as the reduction operation is associative, $(a \circ b) \circ c = a \circ (b \circ c)$. Note that floating-point operations are often close to -- but in general not -- associative, which means that a parallelized code will return slightly different values than the sequential version. Making this assumption effectively constrains the type signature of the reduction to: 

      \begin{ccode}[]
        {haskell}
        reduce' :: (a -> a -> a) -> a -> [a] -> a
      \end{ccode}

By assuming associativity, the sequence may be broken into chunks and each chunk may be processed independently using a generic initial value; this part can be done entirely in parallel. Afterwards, the accumulators for each chunk must be combined using the same reduction operation; this part must be done sequentially. By making the chunk size as large as possible, the parallel fraction of work remains high. This can be implemented using the constructs already introduced: each thread is assigned a subset of the loop iterations and then the local totals are combined using a critical section.

\begin{ccode}[]
{c}
        double arr[8] = {6,2,1,8,4,6,10,9};
        double total = 0;
        #pragma omp parallel num_threads(2)
        {
          double local_total = 0;
          for (int i=0; i<4; i++)
            local_total += arr[omp_get_thread_num()*4 + i];

          #pragma omp critical
          total += local_total;
        }
\end{ccode}

In practice this is burdensome because the code has to support a variable number of iterations and threads, introducing boilerplate and many edge cases. Furthermore, the above code handles the semantics of transforming the loop into a threaded and chunked version manually, but OpenMP's \texttt{parallel for} directive already provides a generic, high-level way of doing this. Rather than reinventing the wheel, OpenMP provides a \texttt{reduction} clause in order to tersely use reductions inside \texttt{parallel for} regions. An example is as follows:

  \inputcode[firstline=6,lastline=14]{c}{src/reduction.c}

In this example, OpenMP is automatically doing all the work to create a private copy of the reduction variable \texttt{sum}, accumulate into it over the loop iterations assigned to each thread, and then accumulate over those into the global reduction variable. The user needs only specify the reduction operation and destination variable. OpenMP supports the following operators by default: \verb#+, *, -, max, min, &, &&, |, ||, ^#. Custom operators may also be defined in terms of a combination operation \mintinline{haskell}{f :: a -> a -> a} and an initial value of \texttt{a} which acts as an identity of the operator, e.g. 0 for \mintinline{c}{+}, or \mintinline{c}{MAXINT} for \mintinline{c}{min}.

\section{Parallelizing a Lattice-Boltzmann simulation}

OpenMP can be used to parallelize some of the computation-intensive loops of a Lattice Boltzmann (LBM) simulation. LBM is a family of numerical methods for simulating fluids using a formulation derived from statistical mechanics. The key idea is to divide the spatial domain into cells and to model the distribution of particle states within each cell. On each timestep, the fraction of particles moving in a certain direction flows into the neighboring cell and collides with the other incoming particles, yielding a new distribution. The macro-level velocities and densities can then be calculated for each cell from the micro-level particle distributions.

This technique stands in marked contrast to the traditional incompressible fluid dynamics techniques, which are based on the Navier-Stokes equations and model the macro-level properties explicitly. LBM has a number of disadvantages in comparison, such as a speed of sound which is coupled to the lattice size, and needing very small timestep sizes for large Mach numbers. However, it has one key advantage: it is highly parallelizable and hence can be scaled to very large problem sizes or run in real-time on a GPU.

The data model used by a basic LBM code is as follows. The domain is represented as a 2D array of cells. Each cell contains a distribution of \emph{fictitious particles} moving in different lattice directions. For a 2D simulation, the particles may flow into any of the 8 neighbors, or stay in place. Thus, each cell contains a vector of 9 floats, each of which contains the mass fraction moving in one direction. The macroscopic velocities and densities are stored in separate 2D arrays of floats of the same size. While 
most cells contain fluid, some contain a rigid obstacle whereas others are used to establish boundary conditions such as inflow and outflow. This is represented by another 2D array containing an enum denoting cell type, called the \emph{flag field}.

The LBM timestep update rewrites the grid of fictitious particles just like a cellular automaton, but there are additional complications necessary to handle obstacles and boundary conditions. The algorithm always starts with and finishes with a valid particle field. The first step is to perform the collision update, which rearranges the particle distributions inside each cell according to a physical model of an elastic collision. Obstacle cells have their particle distributions reset to zero. The next step is streaming, which examines each pair of adjacent cells and swaps the fictitious particle values flowing from one to the other. After the streaming step has taken place, it is necessary to perform one more pass over the spatial domain to re-establish boundary conditions, including \emph{bouncing back} particles which had been streamed into obstacle cells. Finally, the macroscopic quantities may be calculated from the particle distributions.

Several of the aforementioned steps are embarrassingly parallel, and are correspondingly simple to parallelize with OpenMP. These steps write to a given cell without reading from the neighboring cells, which means there is no dependence between them. This is clearly the case for the collision update and the computation of macroscopic values. The code ends up having the following form:

\begin{ccode}[]
{c}
        #pragma omp parallel for schedule(dynamic)
        for (int y = 0; y < height; y++) {
          for (int x = 0; x < width; x++) {
            Cell &cell = particledist.value(x,y);
            Flag flag = flagfield.value(x,y);
            // Update cell _without_ referring to neighbors
          }
        }
\end{ccode}

The streaming step presents an additional complication. Because the new values of each cell depend on the values of their neighbors, there is a write-after-read dependency between cells. There are three main approaches to overcoming this. The first approach is to subdivide the domain into smaller regions, stream each region in a separate thread, and then handle the streaming between subregions afterwards. This approach is identical to the case of distributed-memory parallelization, for example with MPI, and the main downside is the increase in implementation complexity. If this approach is taken, it makes more sense to use MPI instead of OpenMP. MPI-3 can transparently take advantage of shared memory, and unlike OpenMP it scales to many processors.

The second, simpler approach is based on graph coloring: particles that are moving in different directions do not affect each other. Thus, one may parallelize streaming across the 8 different directions of the distribution vector. This loop is normally nested inside the loops over all cells; to increase granularity it should be interchanged so that it is the outermost. There are two downsides. Firstly, this yields a maximum speedup of 8. More significantly, interchanging the loops dramatically increases the memory movement because the temporally local values now occupy separate cache lines. Although one could remedy this by turning the data structure inside-out, doing so would dramatically increase the memory movement of the collision step for similar reasons. 

The best approach is simpler yet. A write-after-read dependency can usually be resolved by moving the write someplace else. By maintaining separate buffers for the old and new cell values, the program is guaranteed to always read the correct (old) value and the writes no longer interfere. The tradeoff is that this requires twice the memory. This approach also allows the boundary condition application to be parallelized, which is significantly more irregular due to the arbitrary obstacle placement.

All of the major compute kernels of the basic Lattice Boltzmann method can be parallelized using only \texttt{parallel for}. The use of dynamic scheduling allows for better load balancing particularly as the number of grid cells grow large. The use of the \texttt{collapse} clause tells OpenMP to parallelize across all spatial dimensions rather simply the outermost, exposing even more parallelism. However, this has consequences: with dynamic scheduling, the threads must pop work off of a hidden queue in a synchronized manner. As the number of cells and the number of threads increases, Amdahl's Law implies that this overhead eventually saturates the speedup. The impact from increasing the number of cells may be ameliorated by increasing the chunk size, but the impact from increasing the number of threads is unavoidable. As a result, LBM scales well as the problem size increases along with the number of cores (\emph{weak scaling}), but scales less well when the problem size stays constant while the number of cores increases (\emph{strong scaling}). 

\section{Conclusion}




\end{document}



