\documentclass[conference, a4paper]{IEEEtran-modified}
\usepackage{tikz}
\usepackage{multicol}
\usepackage[outputdir=build]{minted}
\usepackage{tcolorbox}
\tcbuselibrary{minted,skins}

% Patch to fix https://github.com/T-F-S/tcolorbox/issues/12
\makeatletter
\def\tcb@minted@input@listing#1#2#3#4{%
  \edef\temp@a{#4}%
  \ifx\temp@a\@empty%
  \else%
    \toks@=\expandafter{#4}%
    \edef\tcb@temp{\noexpand\usemintedstyle{\the\toks@}}%
    \tcb@temp%
  \fi%
  \toks@=\expandafter{#1}%
  \edef\tcb@temp{\noexpand\inputminted[\the\toks@]}%
  \IfFileExists{\minted@outputdir#3}%
    {\tcb@temp{#2}{\minted@outputdir#3}}%
    {\tcb@temp{#2}{#3}}%
}
\makeatother

\newtcblisting{ccode}[2][]{%
  listing engine=minted,
  minted language=#2,
  minted options={breaklines,breakanywhere,fontsize=\scriptsize,gobble=8},
  listing only,
  before skip=5pt,
  after skip=5pt,
  left skip=0pt,
  right skip=0pt,
  size=fbox,
  sharp corners,
  %colframe=white!75!black,
  colframe=white,
  boxrule=0pt,
  frame hidden,
  #1
}


\newtcbinputlisting[]{\inputcode}[3][]{%
  listing engine=minted,
  minted language=#2,
  minted options={breaklines,breakanywhere,fontsize=\scriptsize,#1},
  listing file={#3},
  listing only,
  size=fbox,
  before skip=5pt,
  after skip=5pt,
  left skip=0pt,
  right skip=0pt,
  sharp corners,
  %colframe=white!75!black,
  colframe=white,
  boxrule=0pt,
  frame hidden
}


\title{A Brief Introduction to Shared-Memory Parallelization using OpenMP}

\specialpapernotice{Ferienakademie 2017}

\author{
\authorblockN{Nathan W Brei}
\authorblockA{\texttt{brei@in.tum.de}} 
%\and
%\authorblockN{}
%\authorblockA{}
}
\date\today
\pagestyle{plain}

\begin{document}
\maketitle

This document provides a concise introduction to parallelizing a program using OpenMP. OpenMP is an API for shared-memory parallelism, distributing the computation across multiple threads which all access the same address space. It is designed to allow a programmer to easily adapt existing sequential code by annotating blocks which can be parallelized. Each annotation corresponds to a high-level pattern. This paper introduces the most essential patterns, specifically parallel regions, parallel sections, critical/atomic sections, parallel for loops, barriers, and reductions. Along the way it discusses fundamental parallel programming concepts such as race conditions and dependency graphs. In the end, this paper shows how the API may be used to parallelize a Lattice-Boltzmann fluid simulation.

\section{Parallelism basics}

There are two main reasons to parallelize a program: to make it faster, and to handle larger problem sizes. The measure of effectiveness is usually \emph{speedup} $ S(n) = t_1 / t_n$, where $n$ denotes the number of resources, e.g. processors, and $t$ denotes time taken. If a program parallelizes perfectly, then $S(n) = n$. This is only the case for problems which can be divided into completely independent subproblems, which are called \emph{embarrassingly parallel}. Otherwise, the speedup saturates so that adding more resources yields diminishing returns. This is modelled as \emph{Amdahl's Law}, 

\[
S(n) = \frac{1}{1-f+f/n}
\]

where $f$ denotes the fraction of the computation which can be fully parallelized. As $n$ is increased, the runtime becomes dominated by the parts of the program which do not benefit from the parallelism.

When designing parallel programs, it is helpful to imagine an assembly line. Although work may be divided between different processors, some operations must be performed sequentially, creating a \emph{critical path} which acts as an upper bound on the attainable performance, called the \emph{span}. Optimizing operations which are not on the critical path does not improve performance. Thus the bulk of a program does not need to be optimized, but rather just the \emph{bottlenecks}. There are two main kinds of bottlenecks: a \emph{compute bound} is limited by the throughput of executing instructions, whereas a \emph{memory bound} is limited by bandwidth of moving memory into the CPU. According to the \emph{Roofline Model}, these can be distinguished by calculating the number of floating-point operations per byte, known as the \emph{arithmetic intensity}. The general procedure is to identify the key bottleneck and reorganize it until it is no longer the key bottleneck, then repeat.


\section{Shared-memory versus distributed-memory parallelism}

    A program may be parallelized at different levels. \emph{Distributed-memory parallelism} spreads the work across multiple processors which must communicate by sending each other messages. \emph{Thread-level (shared-memory) parallelism} spreads the work across multiple threads within one processor. \emph{Instruction-level parallelism} maximizes the throughput of instructions within the context of each thread. Each of these levels comes with a different set of tradeoffs and tools. They may usually be combined, although this increases the complexity of the program.

\subsection{Distributed-memory parallelism}

    Distributed-memory parallelism splits the computation across multiple processors. These processors may be located within the same physical machine, or they may be geographically far apart. In either case, the program is run as a group of isolated processes which interact by passing messages. The most commonly used framework in scientific computing is MPI, while in other fields frameworks like Hadoop, Spark, and Erlang are popular.

    The main advantage to distributed-memory parallelism is that it permits high scaling. While a single processor may offer compute power in the low teraflops, a supercomputer scales this to the high petaflops. The performance of a single processor is limited by geometry and the physics of heat dissipation, whereas the performance of a cluster is limited mainly by the cost of communication overhead. This reveals a key disadvantage: a network hop is orders of magnitude more expensive than the corresponding memory lookup, leading to a much lower memory bound. The second disadvantage is reliability: as the number of machines increases, so too does the probability that one of the machines fails. A long-running program must account for this; the simplest option is to periodically save a checkpoint containing the program's state. Frameworks such as Spark automatically detect failures and rearrange the computation to compensate. 


\subsection{Instruction-level parallelism}

    Instruction-level parallelism happens within the context of a single core, and is specific to each processor's microarchitecture. On one hand, each core maximizes its own instruction throughput by pipelining and parallelizing functional units. On the other hand, the use of vectorized \emph{SIMD} (Single Instruction, Multiple Data) instructions allows one instruction to operate on multiple values at once. Taking advantage of this parallelism level requires detailed knowledge of the hardware, so it is usually delegated to the compiler. The programmer usually coaxes the compiler to find an efficient solution by reorganizing data structures and providing hints. Although this maximizes the utilization of each core, there is a limited speedup possible (e.g. 8) and even this requires the problem to be compute-bound. Tools for exploiting instruction-level parallelism include pragmas, intrinsics functions, and plain assembly code.


\subsection{Shared-memory thread-level parallelism}

    Thread-level parallelism splits the computation across multiple threads within a single processor. This can yield a speedup when each thread is executed on a separate core. Each thread sees its own isolated register space, but all threads access the same memory address space. The standard POSIX threading library is called pthreads, but it is too low-level for everyday use. Two popular higher-level libraries are OpenMP and Thread Building Blocks (TBB). OpenMP extends C/C++/Fortran using pragmas, whereas TBB is built from C++ templates.

    The main advantage to thread-level parallelism is that the same data is automatically available to all threads. This makes it the only parallelism level which is effective for memory-bound problems: distributed-memory parallelism narrows the memory bottleneck, and instruction-level parallelism is overshadowed by it. A second advantage is that it allows an incremental approach to parallelizing existing code. In contrast, distributed-memory parallelism requires reorganizing the program's entire control flow, and instruction-level parallelism often requires reorganizing the data structures. 

    The main disadvantage of thread-level parallelism is that it is conceptually tricky -- for both a human and a compiler -- to reason about program correctness and liveness. A program may intermittently return incorrect results or hang, depending on the exact pattern of data updates. The high-level parallelism directives can not completely isolate the effects of their enclosed computations. In order to write correct parallel code, a programmer must have a thorough understanding of the sequential code. Furthermore, flaws usually cannot be detected at compile time. Tools such as ThreadSanitizer \emph{instrument} the binary in order to detect them at runtime.


\section{Introduction to OpenMP}

    OpenMP provides high-level threading constructs as a language extension to C, C++, and Fortran. It develops a set of parallel programming patterns and allows the user to apply these patterns throughout an existing codebase.

    Tackling the problem at a higher level allows OpenMP to assuage many of the human-factors problems associated with multithreaded programming. The boilerplate for managing threads is long, tedious, and error-prone. Because this code has been consolidated, it can handle edge cases thoroughly and with high generality. For instance, not only is it is possible to set the number of threads at runtime, but the optimal number can be determined automatically by querying the operating system. This implies that OpenMP offers automatic fallback to sequential code.  

    There are several downsides associated with using OpenMP. Due to the use of compiler directives, the parallelized version of the code is kept hidden from the user. The only way to understand the transformations OpenMP applied to the code is by using a specialized debugger, or reading the assembly. This can be immediately compared with languages such as Julia, which implement similar functionality using macros, thereby allowing the code to be inspected after the transformations are applied but before they are compiled.

    A second downside is that the OpenMP syntax remains simple even if the resulting transformation is quite complex. To a programmer, this means that parallelizing each code region is an all-or-nothing task. To a compiler, this means that there can be very limited error detection. It can detect when directives are nested in a way which does not make sense, or when the arguments to a clause are invalid. However, there remains a huge space of programs which are syntactically valid but semantically incorrect. OpenMP by its very nature transforms the meaning of a program; it does not attempt to preserve that program's correctness in the process.

    The OpenMP syntax has a simple grammar, as shown by the example below:
      \begin{ccode}[]
        {text}
        #pragma omp parallel private(acc) num_threads(8) 
        {
            // Some code
        }\end{ccode}

    \texttt{\#pragma} indicates that the rest of the line is not C code, but rather an optional hint for the compiler. \texttt{omp} is a \emph{sentinel} indicating that this is an OpenMP directive. \texttt{parallel} is the directive name, which in this case indicates that the following block of code should be run in parallel using a team of threads, as described in the next section. The directive name is followed by a set of clauses. Clauses are separated by spaces, may be in any order, and may be repeated. Many clauses take parameters, which are enclosed in parentheses following the clause name; arguments are separated by commas and keyworded arguments by colons, similar to Python. Blocks of code must be in brackets, and the opening bracket must be on the following line, as otherwise it gets parsed as being part of the pragma.
    


\section{Launching a team of threads}

    The most basic OpenMP construct is the \emph{parallel region}, which is a single block of code that gets executed once by every thread in the team. This implements the \emph{fork-join execution model}, in which control flow starts in a single thread, forks to multiple threads, all of which perform some computation, and then rejoins upon completion, with one thread continuing execution and the rest either idling or terminating. The following example shows an extremely simple block run within a parallel region that has been constrained to use three threads:

      \inputcode[]{c}{src/ex1.c}

    The \texttt{num\_threads} clause fixes the number of threads in the team. This should only be hardcoded like this when the correctness of the enclosed code depends on that number, since hardcoding prevents the program from taking advantage of additional cores which may be available on different machines. In order to set the value at runtime from within the program, OpenMP also provides the function \mintinline{c}{omp_set_num_threads(int t)}. To set the value at runtime from outside the program it is convenient to use the environment variable \mintinline{c}{OMP_NUM_THREADS=3}. If the variable is not constrained anywhere, OpenMP will query the operating system in order to determine the best value. 


    It is important to note that the OpenMP specification does not guarantee that three threads are used in practice, or even that the code is run in parallel at all. If the implementation decides that the best option is to run the code sequentally within one thread three times, it is free to do so. This is for practical reasons: the mapping from threads to cores is buried beneath many layers, including the OpenMP implementation, the operating system, and the CPU architecture, all of which may vary independently and offer their own notion of a virtualized thread.

    This does not pose a theoretical problem because while instructions within a thread are required to be ordered, there is no corresponding assumption between threads. Instead, instructions in different threads get interleaved nondeterministically. For instance, two runs of the above code yield different outputs:

\begin{multicols}{2}
      \inputcode[]{text}{output/ex1.txt}

      \begin{ccode}[]
        {text}
        About to fork...
        Hello from thread 2 of 3
        Hello from thread 0 of 3
        Hello from thread 1 of 3
        ...Rejoined.\end{ccode}
\end{multicols}

    Note however that the interleaving is also not always obviously random. Small examples, such as those in this paper, often run with the same interleaving many times in a row, and different interleavings only manifest when run on different hardware or on a larger problem size. This is because the interleaving depends on the CPU architecture, operating system, system load, compiler, and code optimization level. This means that unit testing is ineffective at finding threading bugs, and the programmer should use tools like Valgrind or ThreadSanitizer instead. 


\section{Sections}


Oftentimes a programmer wishes to assign distinct code blocks to individual threads. While this could technically be accomplished by using a plain parallel region and branching according to thread number, a higher-level construct, called a \emph{section}, achieves this in a general way. 

The syntax is as follows: Within a \texttt{parallel} region, the programmer may declare a \texttt{sections} region. Within the latter, they may then declare any number of individual \texttt{section} blocks. Each \texttt{section} is guaranteed to be executed exactly once, and OpenMP automatically decides how sections get mapped to threads. Note that the number of sections is finite and fixed at compile time. For brevity the \texttt{parallel} and \texttt{sections} directives may be combined as shown in the example below: 

      \inputcode[]{c}{src/ex2.c}
      \begin{multicols}{2}

      \begin{ccode}[]
        {text}
        Thread 0, iter 0
        Thread 0, iter 1
          Thread 1, iter 0
        Thread 0, iter 2
          Thread 1, iter 1
          Thread 1, iter 2
          Thread 1, iter 3
        Thread 0, iter 3\end{ccode}
      \inputcode[]{text}{output/ex2.txt}
\end{multicols}

At the end of a \texttt{sections} region is an \emph{implicit barrier}. A barrier is a point in the program execution which no thread may cross until all other threads have reached that point. This dramatically simplifies the program state space, making the program easier to reason about. It also enforces a coarse ordering of instructions between threads: Instructions on one side of the barrier will always execute before instructions on the other, regardless of thread. The downsides of using them are that they introduce additional overhead, reduce parallelism, and require finer-grained load balancing. Barriers can be placed inside a parallel region as follows:

      \begin{ccode}[]
        {c}
        #pragma omp barrier \end{ccode}


While barriers are used explicitly and often when programming with pthreads, they rarely need to be explicitly invoked with OpenMP, because they are baked into many of OpenMP's higher-level work-sharing constructs. Instead, the usual workflow has the programmer parallelizing the code without worrying about barriers, profiling the code, and then disabling the barriers that create hot spots. This is done by specifying the clause \texttt{nowait}. It is up to the programmer to ensure that disabling the barriers does not affect the correctness of the program.


\section{Race conditions}

The previous examples demonstrated that, in the absence of barriers, instructions are only \emph{partially ordered}: Within each thread all instructions are ordered, but between threads they may be interleaved nondeterministically like a shuffled deck of cards. If multiple threads access a shared resource, the ordering of those accesses is unpredictable. This makes multithreaded programs vulnerable to \emph{race conditions}, a class of bug where the program's output depends on the timing of events which are not under the programmer's control. The following code snippet shows a variable \texttt{b} being assigned the value of \texttt{a}, while \texttt{a} is being simultaneously updated. Whether the new value of \texttt{a} propagates into \texttt{b} is indeterminate.


      \begin{ccode}[]{c}
        int a = 0;
        int b = 0;
        #pragma omp parallel sections
        {
          #pragma omp section
          a = 1;

          #pragma omp section
          b = a;
        }
        printf("b = %d\n", b);\end{ccode}


Oftentimes race conditions are not obvious from the C code, but emerge from the resulting assembly or microcode. One major reason is that the compiler determines when data gets moved between memory and registers. The programmer must assume that every variable access requires separate \texttt{load} and \texttt{store} operations which may be vulnerable to interleaving. It is possible to avoid this by using \emph{atomic} operations, but this is very inefficient, for reasons which shall be explored later. 

The following C code updates a shared accumulator, \texttt{total}, in two separate threads. At first glance it appears that this code will always return 3, regardless of interleaving, because integer addition is associative.

%  \inputcode[firstline=5,lastline=22,gobble=2]{c}{src/ex4.c}
        \begin{ccode}[]{c}
        #include<stdio.h>
        #include<omp.h>
        int main(int argc, char** argv) {

          int total = 0;
          #pragma omp parallel sections
          {
            #pragma omp section
            {
              total = total + 1;
            }

            #pragma omp section
            {
              total = total + 2;
            }
          }
          printf("total=%d\n",total);
        }
\end{ccode}



However, once each statement has been broken into separate \texttt{load}, \texttt{add}, and \texttt{store} statements, it becomes clear that there exist interleavings which are not correct. To run correctly, the instruction ordering must be constrained such that one \texttt{load} comes after the other \texttt{store} instruction; otherwise the effect of one of the statements will be overwritten completely.

\inputcode[firstline=5,lastline=22,gobble=2]{c}{src/ex5.c}

\begin{multicols}{2}
  
      \begin{ccode}[after skip=4pt]
        {text}
        load (total) -> a
          load (total) -> c
        add 1, a -> b
          add 2, c -> d
        store b -> (total)
          store d -> (total)

          ==> (total == 2)\end{ccode}
      \begin{ccode}[]
        {text}
        load (total) -> a
        add 1, a -> b
          load (total) -> c
          add 2, c -> d
          store d -> (total)
        store b -> (total)
        
          ==> (total == 1)\end{ccode}
\end{multicols}

In a sequential program, some instructions can be reordered without changing the meaning of the program, whereas others must not. In a parallel program, certain instructions will be reordered. Thus for a parallel program to have the same output as the sequential version, the space of possible reorderings must exclude any which are not compatible with the sequential version. This is expressed formally through \emph{dependency analysis}. The basic idea is to represent the minimal necessary ordering constraints between instructions (\emph{dependencies}) as a graph and then ensure that the parallel version preserves the graph structure. 

There are two broad kinds of dependencies, data dependencies and control dependencies. Control dependencies arise at conditional jumps, which need the results of a previous instruction to determine what the next instruction should be. Data dependencies arises from modifying the contents of variables; two instructions have a data dependency if both access the same variable and at least one access is a write. Data dependencies may be subdivided into three subgroups: Read-After-Write (RAW), Write-After-Read (WAR), and Write-After-Write (WAW). These are illustrated below:

      \begin{ccode}[]
        {c}
        1: x = 22;
        2: y = x + 1;     // RAW dep on (1)
        3: x = 0;         // WAR dep on (2)
        4: y = 0;         // WAW dep on (3)\end{ccode}


RAW dependencies are the most difficult to deal with; the programmer must either serialize the dependence or change the entire algorithm. On the other hand, WAR and WAW dependencies can often be finessed away by simply writing to a different memory location. This forms the basis of \emph{register renaming}, a technique used inside CPUs for increasing instruction-level parallelism. 


\section{Privatizing intermediate variables}

  One very common source of errors in OpenMP programs is that the compiler treats variables as shared by default. In other words, if a variable is declared outside of the \texttt{parallel} or \texttt{sections} directive, and is used inside it, the compiler assumes that the symbol refers to the same memory location throughout. In some cases, particularly with iteration or temporary variables, this creates a spurious dependence. In the following code snippet, one thread attempts to count from 0 to 3 and the other attempts to count from 4 to 7. The variables $i$ and $t$ are declared outside of the \texttt{sections} directive. 


  \inputcode[firstline=8,lastline=32,gobble=2]{c}{src/ex7.c}

  The correctness of this code hinges on the clause \mintinline{c}{private(i,t)}. When included, the program runs as expected.

  \inputcode[]{text}{output/ex7.txt}

  When the clause is omitted, the threads overwrite the variables in a chaotic way, causing the loops to terminate early:

  \inputcode[]{text}{output/ex6.txt}

  When writing new code, it is good practice to declare thread-local variables within the block itself and to avoid writing to global state. With existing code, this would require significant refactoring and is sometimes not even possible. The alternative is to explicitly tell OpenMP which variables are meant to be shared and which are meant to be isolated. There are several clauses for this purpose. By default, OpenMP assumes that all variables are shared, and the programmer must declare them as \mintinline{c}{private}. This default assumption may be changed, via the \mintinline{text}{default} clause, which accepts as arguments \mintinline{text}{shared} or  
  mintinline{c}{none}. When the default is changed to \mintinline{text}{none}, all variables used within the parallel region must be explicitly scoped. 

  There are some subtleties involved when scoping a variable as \mintinline{text}{private}. Under the hood, OpenMP is declaring a new object of the same type for each thread and changing all references within the block to refer to it. However, it neither initializes the local variables nor updates the global variable after the parallel region has exited. For the former, OpenMP provides the \mintinline{c}{firstprivate} clause, which copies the global value into each local variable. For the latter, it provides the \mintinline{c}{lastprivate} clause, which assigns the local value from the last loop iteration or section to the global variable. This mimics the effects of the sequential version of the code. The two clauses may be combined.

\section{Protecting shared resources}

  While it is desirable to keep variables private to a thread whenever possible, results must be written to a shared variable at some point, as otherwise the results of the computation will be lost when the threads join. The standard tools for achieving this are locks and mutexes. The basic idea is that each shared variable (or `resource') is protected by a lock variable. Each thread should acquire the lock before modifying the resource and release it afterwards. If one thread has already acquired the lock, the other threads must wait until it is released. This basic pattern forms the core of the \texttt{pthreads} library. 

  Unfortunately, using locks in practice is difficult to reason about and prone to bugs. OpenMP provides an interface for working with locks if necessary, but it strongly prefers a higher-level construct called a \emph{critical section}. Critical sections operate on explicit code blocks, rather than on the abstract notion of a resource. OpenMP sets up locks under the hood to ensure that only one thread may be inside a critical section at any given time. The programmer only needs to make sure that all writes to the shared resource are inside critical sections, and the resource will be protected.

  The downside to using critical sections is that they effectively serialize part of the program. As per Amdahl's Law, the larger the critical sections, the worse the program will scale. Thus it is desirable to keep them as short and as few as possible. If there are multiple resources which can be updated independently, it is beneficial to use \emph{named} critical sections for each resource, so that updates to one resource don't unnecessarily block updates to another.

  Oftentimes the critical sections are extremely short, possibly only several assembly instructions, such as the summation operator \texttt{+=}. In this case, it is overkill to acquire a lock since the synchronization can be achieved by protecting the load and store at the assembly level instead. This is much finer-grained and lightweight. However, it may only be used on statements with unary operators such as \mintinline{c}{++, --} or binary operators such as \mintinline{c}{+=, *=, -=, /=, &=, ^=, |=, <<=, >>=}. Use of a named critical section and atomic section are shown below.

    \inputcode[firstline=6,lastline=28,gobble=2]{c}{src/ex8.c}


\section{Parallel For Loops}

  For numerical code, the majority of the computation usually happens inside a loop, and the programmer wishes to distribute the work evenly across the machine's cores. This should not be achieved with sections because sections use a fixed number of threads and would require code duplication for each thread. Instead, OpenMP provides a directive to parallelize a loop directly. The following code reports which thread handled each iteration number:

      \inputcode[]{c}{src/exfor.c}
      
  This mapping is called the \emph{scheduling strategy}, and OpenMP offers three options: static, dynamic, and guided. Static scheduling assigns each thread a chunk of work in a round-robin fashion. For instance, if the chunk size is 3, then thread 0 takes the first 3 iterations, thread 1 takes the next 3, etc. Dynamic scheduling also divides the work into chunks of constant size, but assigns chunks to threads on a first-in, first-out basis, which results in better load balancing at the expense of extra overhead. Guided scheduling is like dynamic except that it starts with a large chunk size and decreases it over time. 

  The strategy may be specified as a clause or chosen at runtime. The static strategy works best when each iteration performs the same amount of work and the number of threads evenly divides the number of iterations. Otherwise, the strategy can be chosen experimentally. Meanwhile, small chunk sizes lead to better load balancing, whereas large chunk sizes lead to coarser parallelism and hence less overhead.


\section{Loop-Carried Dependencies}

  The dependency analysis described earlier can be extended to handle the case of loops. There are two main goals: firstly, to determine whether a loop can be parallized as-is, and secondly, if not, to find a transformation which allows it to be parallelized efficiently. A loop may be parallelized if there are no dependencies between statements in different iterations, known as \emph{loop-carried dependencies}. One simple heuristic is whether the loop can be easily rewritten using the \mintinline{c}{map()} function:

        \begin{ccode}[]
        {haskell}
        map :: (x -> y) -> [x] -> [y]\end{ccode}

If the loop has carried dependencies, it may be helpful to apply certain loop transformations. One technique is \emph{loop splitting}, which breaks a loop into two loops, one following the other. This ensures that every statement within the first loop executes before every statement in the second. A similar technique is \emph{loop alignment}, which shifts which piece of data gets updated on each iteration. Both of these techniques can remove dependencies which are accidental to the serial implementation but not fundamental to the underlying algorithm.

  With nested loops, there is the possibility of \emph{interchanging} the nesting order, which is a powerful tool for exposing parallelism when the previous techniques fail. The programmer must make two choices: the new nesting order, and which loop level gets parallelized. There are two goals: to keep loop-carried dependencies within each thread (so that they run sequentially and hence correctly), and to assign each thread as much work as possible for each iteration (to eliminate overhead). The following example compares different options for computing the sum of all rows in a matrix.

      \inputcode[firstline=6,lastline=15,gobble=2]{c}{src/lcd.c}
      \inputcode[firstline=6,lastline=16,gobble=2]{c}{src/lcd2.c}

      \inputcode[firstline=6,lastline=15,gobble=2]{c}{src/lcdbroken.c} 
      \inputcode[firstline=6,lastline=15,gobble=2]{c}{src/lcdbroken2.c} 

  The $i$ loop has loop-carried dependencies, whereas the $j$ loop does not. Thus the \texttt{parallel for} directive should be applied to the $j$ loop; otherwise the program would be incorrect. If the $j$ loop is on the inside, there are $m$ parallel forks/joins, each of which performs one instruction. On the other hand, if it is outermost, there is only one parallel fork/join, which performs $m$ operations. Thus one choice stands out as best.


\section{Reductions}

While \emph{embarrassingly parallel} loops fit the pattern used by the \mintinline{c}{map()} function, where each iteration in the loop is completely independent, other loops more closely resemble its counterpart \mintinline{c}{reduce()}, where an accumulator is updated with a new piece of data on every iteration. The canonical example of a reduction is summing the elements in an array. Both of these patterns can be understood from their type signatures: \mintinline{c}{reduce()} takes a combining operation, an initial value for the accumulator, and a sequence of values, and returns the accumulator after it has been combined with the entire sequence.

      \begin{ccode}[]
        {haskell}
        reduce :: (a -> x -> a) -> a -> [x] -> a\end{ccode}

Although a reduction is practically the definition of a loop-carried dependence, it may nonetheless be parallelized as long as the reduction operation is associative, $(a \circ b) \circ c = a \circ (b \circ c)$. (Be aware that floating-point operations are close to -- but in general not -- associative, which means that a parallelized code will return results with slightly different rounding errors.) Assuming associativity constrains the type signature of the reduction to: 

      \begin{ccode}[]
        {haskell}
        reduce' :: (a -> a -> a) -> a -> [a] -> a\end{ccode}

This lets the sequence be broken into chunks that may be processed in parallel using a generic initial value. Afterwards, the accumulators for each chunk must be combined using the same reduction operation; this part must be done sequentially. By making the chunk size large, however, the parallel fraction of work remains high. This can be implemented using the constructs already introduced: each thread is assigned a subset of the loop iterations and then the local totals are combined using a critical section.

\begin{ccode}[]
{c}
        double arr[8] = {6,2,1,8,4,6,10,9};
        double total = 0;
        #pragma omp parallel num_threads(2)
        {
          double local_total = 0;
          for (int i=0; i<4; i++)
            local_total += arr[omp_get_thread_num()*4 + i];

          #pragma omp critical
          total += local_total;
        }
\end{ccode}

In practice this is burdensome because the code has to support a variable number of iterations and threads, introducing boilerplate and many edge cases. Furthermore, the above code handles the semantics of transforming the loop into a threaded and chunked version manually, but OpenMP's \texttt{parallel for} directive already provides a generic, high-level way of doing this. Rather than reinventing the wheel, OpenMP provides a \texttt{reduction} clause in order to tersely use reductions inside \texttt{parallel for} regions. An example is as follows:

  \inputcode[firstline=6,lastline=14]{c}{src/reduction.c}

In this example, OpenMP is automatically doing all the work to create a private copy of the reduction variable \texttt{sum}, accumulate into it over the loop iterations assigned to each thread, and then accumulate over those into the global reduction variable. The user needs only specify the reduction operation and destination variable. OpenMP supports the following operators by default: \verb#+, *, -, max, min, &, &&, |, ||, ^#. Custom operators may also be defined in terms of a combination operation \mintinline{haskell}{f :: a -> a -> a} and an initial value of \texttt{a} which acts as an identity of the operator, e.g. 0 for \mintinline{c}{+}, or \mintinline{c}{MAXINT} for \mintinline{c}{min}.

\section{Parallelizing a Lattice-Boltzmann simulation}

OpenMP can be used to parallelize some of the computation-intensive loops of a Lattice Boltzmann (LBM) simulation. LBM is a family of numerical methods for simulating fluids using a formulation derived from statistical mechanics. The key idea is to divide the spatial domain into cells and to model the distribution of particle states within each cell. On each timestep, the fraction of particles moving in a certain direction flows into the neighboring cell and collides with the other incoming particles, yielding a new distribution. The macro-level velocities and densities can then be calculated for each cell from the micro-level particle distributions.

This technique stands in marked contrast to the traditional incompressible fluid dynamics techniques, which are based on the Navier-Stokes equations and model the macro-level properties explicitly. LBM has a number of disadvantages in comparison, such as a speed of sound which is coupled to the lattice size, and needing very small timestep sizes for large Mach numbers. However, it has one key advantage: it is highly parallelizable and hence can be scaled to very large problem sizes or run in real-time on a GPU.

The data model used by a basic LBM code is as follows. The domain is represented as a 2D array of cells. Each cell contains a distribution of \emph{fictitious particles} moving in different lattice directions. For a 2D simulation, the particles may flow into any of the 8 neighbors, or stay in place. (This is called D2Q9 in literature.) Each cell contains a vector of 9 floats, each of which contains the mass fraction moving in one direction. The macroscopic velocities and densities are stored in separate 2D arrays of floats of the same size. While most cells contain fluid, some contain a rigid obstacle, and others establish boundary conditions such as inflow and outflow. This is represented by another 2D array containing an enum denoting cell type, called the \emph{flag field}.

The LBM timestep update rewrites the grid of fictitious particles just like a cellular automaton, but there are additional complications necessary to handle obstacles and boundary conditions. The algorithm always starts with and finishes with a valid particle field. The first step is to perform the collision update, which rearranges the particle distributions inside each cell according to a physical model of an elastic collision. Obstacle cells have their particle distributions reset to zero. The next step is streaming, which examines each pair of adjacent cells and swaps the fictitious particle values flowing from one to the other. After the streaming step has taken place, it is necessary to perform one more pass over the spatial domain to re-establish boundary conditions, including \emph{bouncing back} particles which had been streamed into obstacle cells. Finally, the macroscopic quantities may be calculated from the particle distributions.

Several of the aforementioned steps are embarrassingly parallel, and are correspondingly simple to parallelize with OpenMP. These steps write to a given cell without reading from the neighboring cells, which means there is no dependence between them. This is clearly the case for the collision update and the computation of macroscopic values. The code ends up having the following form:

\begin{ccode}[]
{c}
        #pragma omp parallel for schedule(dynamic) collapse(2)
        for (int y = 0; y < height; y++) {
          for (int x = 0; x < width; x++) {
            Cell &cell = particledist.value(x,y);
            Flag flag = flagfield.value(x,y);
            // Update cell _without_ referring to neighbors
          }
        }
\end{ccode}

The streaming step presents an additional complication. Because the new values of each cell depend on the values of their neighbors, there is a write-after-read dependency between cells. There are three possible approaches to overcoming this. The first approach is to subdivide the domain into smaller regions, handle each region in a separate thread, and then handle the streaming between subregions afterwards. This approach is identical to the case of distributed-memory parallelization, for example with MPI, and the main downside is the increase in implementation complexity. If this approach is taken, it makes more sense to use MPI instead of OpenMP. MPI-3 can transparently take advantage of shared memory, and unlike OpenMP it scales to many processors.

The second, simpler approach is based on graph coloring: particles that are moving in different directions do not affect each other. Thus, one may parallelize streaming across the 8 different directions of the distribution vector. This loop is normally nested inside the loops over all cells; to increase granularity it should be interchanged so that it is the outermost. There are two downsides. Firstly, this yields a maximum speedup of 8. More significantly, interchanging the loops dramatically increases the memory movement because the temporally local values now occupy separate cache lines. Although one could remedy this by turning the data structure inside-out, doing so would dramatically increase the memory movement of the collision step for similar reasons. 

The best approach is simpler yet. A write-after-read dependency can usually be resolved by moving the write someplace else. By maintaining separate buffers for the old and new cell values, the program is guaranteed to always read the correct (old) value and the writes no longer interfere. The tradeoff is that this requires twice the memory. Unlike the graph coloring approach, this also allows the boundary condition application to be parallelized, which is significantly more irregular due to the arbitrary obstacle placement.

All of the major compute kernels of the basic Lattice Boltzmann method can be parallelized using only \texttt{parallel for}. The use of dynamic scheduling allows for better load balancing particularly as the number of grid cells grow large. The use of the \texttt{collapse} clause tells OpenMP to parallelize across both spatial dimensions rather simply the outermost. However, decreasing the parallelism granularity has consequences. With dynamic scheduling, the threads must pop work off of a hidden queue in a synchronized manner. As the number of cells and the number of threads increases, Amdahl's Law implies that this overhead eventually saturates the speedup. The impact from increasing the number of cells may be ameliorated by increasing the chunk size, but the impact from increasing the number of threads is unavoidable. As a result, LBM scales very well as the problem size increases along with the number of cores (\emph{weak scaling}), but scales somewhat less well when the problem size stays constant while the number of cores increases (\emph{strong scaling}). 

\section{Conclusion}

This paper provided a whirlwind introduction to parallel programming using OpenMP with an emphasis on numerical code. To go further, there are several options. LLNL \cite{LLNL} provides an OpenMP tutorial which goes into much greater depth but still remains concise. On the other hand, Divarkar et al \cite{Viswanath:2017:SPC:3152599} provide a much broader introduction to scientific computing which explore a variety of low-level programming techniques and relate their effectiveness back to computer architecture considerations. Finally, the OpenMP committee itself \cite{OMP} provides a variety of helpful resources.

OpenMP goes well beyond the functionality discussed here. The most important omission is \emph{tasking}, which overcomes the limitations of requiring a finite number of sections or a known number of loop iterations. It allows the programmer to specify tasks, optionally dependent on other tasks, which get dynamically processed by a team of threads via a task queue. This is particularly useful for parallelizing recursive algorithms. Its main downsides are that it is more complex for the programmer to reason about, and harder to obtain a good speedup in practice.

The newer versions of OpenMP also support heterogeneous systems (e.g. offloading work to a GPU or other accelerator), thread affinity (for non-cache coherent multicore processors), and SIMD directives (to mix instruction-level parallelism with thread-level parallelism in a unified way). These features can provide further refinements to codes which have been parallelized using the basic techniques outlined here.


\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,references}
\newpage


\end{document}



